{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import tensorflow.compat.v1 as tf\n",
    "import pandas as pd \n",
    "#from tensorflow import keras\n",
    "#from keras import optimizers\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import TimeDistributed,Input\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import Callback,ReduceLROnPlateau,EarlyStopping,LambdaCallback,TensorBoard\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "from tensorflow.keras.callbacks import History \n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import lsqr\n",
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "import random \n",
    "from scipy.constants import Planck\n",
    "from scipy.optimize import least_squares,minimize\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "import itertools\n",
    "from itertools import product\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from qutip import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from Functions import *\n",
    "from Classes import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "n=1\n",
    "d=np.power(2,n)\n",
    "os.chdir('/home/andrius/Desktop/RNN Estados ML Jeff/Dados')\n",
    "theta=np.loadtxt('angulos.txt')\n",
    "\n",
    "txt = open('(0)(0)ML.txt').read()\n",
    "txt = [item.split() for item in txt.split('\\n\\n')]\n",
    "estado00=text_to_rho(txt,d)\n",
    "estado00_qutip=text_to_rho_qutip(text_to_rho(txt,d),d)\n",
    "#Aplicando produto tensorial com estado |0> \n",
    "#conforme experimento do jeferson\n",
    "#for i in range(len(estado00_qutip)):\n",
    "#    estado00_qutip[i]=tensor(estado00_qutip[i],zero)\n",
    "\n",
    "txt = open('(1)(0)ML.txt').read()\n",
    "txt = [item.split() for item in txt.split('\\n\\n')]\n",
    "estado_10=text_to_rho(txt,d)\n",
    "estado_10_qutip=text_to_rho_qutip(text_to_rho(txt,d),d)\n",
    "#for i in range(len(estado00_qutip)):\n",
    "#    estado_10_qutip[i]=tensor(estado_10_qutip[i],zero)\n",
    "\n",
    "txt = open('estado1ML.txt').read()\n",
    "txt = [item.split() for item in txt.split('\\n\\n')]\n",
    "estado1=text_to_rho(txt,d)\n",
    "estado1_qutip=text_to_rho_qutip(text_to_rho(txt,d),d)\n",
    "#for i in range(len(estado1_qutip)):\n",
    "#    estado1_qutip[i]=tensor(estado1_qutip[i],zero)\n",
    "\n",
    "\n",
    "txt = open('estado2ML.txt').read()\n",
    "txt = [item.split() for item in txt.split('\\n\\n')]\n",
    "estado2=text_to_rho(txt,d)\n",
    "estado2_qutip=text_to_rho_qutip(text_to_rho(txt,d),d)\n",
    "#for i in range(len(estado1_qutip)):\n",
    "#    estado2_qutip[i]=tensor(estado2_qutip[i],zero)\n",
    "\n",
    "\n",
    "txt = open('estado3ML.txt').read()\n",
    "txt = [item.split() for item in txt.split('\\n\\n')]\n",
    "estado3=text_to_rho(txt,d)\n",
    "estado3_qutip=text_to_rho_qutip(text_to_rho(txt,d),d)\n",
    "#for i in range(len(estado3_qutip)):\n",
    "#    estado3_qutip[i]=tensor(estado3_qutip[i],zero)\n",
    "\n",
    "\n",
    "txt = open('estado4ML.txt').read()\n",
    "txt = [item.split() for item in txt.split('\\n\\n')]\n",
    "estado4=text_to_rho(txt,d)\n",
    "estado4_qutip=text_to_rho_qutip(text_to_rho(txt,d),d)\n",
    "#for i in range(len(estado4_qutip)):\n",
    "#   estado4_qutip[i]=tensor(estado4_qutip[i],zero)\n",
    "\n",
    "\n",
    "txt = open('estado5ML.txt').read()\n",
    "txt = [item.split() for item in txt.split('\\n\\n')]\n",
    "estado5=text_to_rho(txt,d)\n",
    "estado5_qutip=text_to_rho_qutip(text_to_rho(txt,d),d)\n",
    "#for i in range(len(estado5_qutip)):\n",
    "#    estado5_qutip[i]=tensor(estado5_qutip[i],zero)\n",
    "\n",
    "\n",
    "txt = open('estado6ML.txt').read()\n",
    "txt = [item.split() for item in txt.split('\\n\\n')]\n",
    "estado6=text_to_rho(txt,d)\n",
    "estado6_qutip=text_to_rho_qutip(text_to_rho(txt,d),d)\n",
    "#for i in range(len(estado6_qutip)):\n",
    "#    estado6_qutip[i]=tensor(estado6_qutip[i],zero)\n",
    "\n",
    "\n",
    "lista_estados=[estado1,estado2,estado3,estado4,estado5,estado6,estado00,estado_10]\n",
    "lista_estados_qutip=[estado1_qutip,estado2_qutip,estado3_qutip,estado4_qutip,estado5_qutip,estado6_qutip,estado00_qutip,estado_10_qutip]\n",
    "#lista_estados_random=random.sample(lista_estados,len(lista_estados))\n",
    "estados=np.array(np.stack((lista_estados),axis=0))\n",
    "\n",
    "\n",
    "verificarHermit(lista_estados_qutip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para escolher como dividir os dados de treino, temos as seguintes opÃ§oes:\n",
    "1- Como eu sempre fiz: Dividir x estados de treino e y de teste, com x+y= numero de estados medidos\n",
    "    i-fornece val_loss referente a tipo de estados nao vistos pela rede\n",
    "2- Usar todos estados e separar para o teste os dados temporais finais de cada estado\n",
    "    i-fornece val_loss referente a steps nao vistos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split=2\n",
    "n_steps=17\n",
    "\n",
    "A_op=np.array(qeye(d))\n",
    "divid=np.int((d**2))\n",
    "data_dim=divid*2\n",
    "\n",
    "rho=estados[:-train_test_split,:,:]\n",
    "rho_test=estados[-train_test_split:,:,:]\n",
    "\n",
    "#Mudar estados selecionados para teste para ver como val loss se comporta\n",
    "#rho_test,rho=np.split(estados,[3],axis=0)\n",
    "\n",
    "x_train,y_train,x_test,y_test=dataSplit(estados,train_test_split,n_steps)\n",
    "data=np.stack([x_train,y_train],axis=-1)\n",
    "data_test=np.stack([x_test,y_test],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(312, 17, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(104, 17, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Epoch 1/50\n",
      "(None, None, 2, 2)\n",
      "(None, None, 2, 2)\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.0426(None, None, 2, 2)\n",
      "19/19 [==============================] - 7s 82ms/step - loss: 0.0425 - val_loss: 0.0271\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0419 - val_loss: 0.0268\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 0.0417 - val_loss: 0.0271\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0416 - val_loss: 0.0269\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0414 - val_loss: 0.0267\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0413 - val_loss: 0.0266\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.0411 - val_loss: 0.0266\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 1s 59ms/step - loss: 0.0407 - val_loss: 0.0274\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 1s 59ms/step - loss: 0.0407 - val_loss: 0.0264\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 1s 55ms/step - loss: 0.0407 - val_loss: 0.0260\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 1s 49ms/step - loss: 0.0405 - val_loss: 0.0259\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.0401 - val_loss: 0.0259\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 1s 57ms/step - loss: 0.0399 - val_loss: 0.0255\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 1s 51ms/step - loss: 0.0400 - val_loss: 0.0281\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 1s 55ms/step - loss: 0.0398 - val_loss: 0.0265\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 1s 57ms/step - loss: 0.0396 - val_loss: 0.0267\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.0394 - val_loss: 0.0258\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0395 - val_loss: 0.0273\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 1s 48ms/step - loss: 0.0389 - val_loss: 0.0320\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 1s 48ms/step - loss: 0.0395 - val_loss: 0.0298\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 1s 49ms/step - loss: 0.0388 - val_loss: 0.0330\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.0389 - val_loss: 0.0319\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0392 - val_loss: 0.0284\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 1s 47ms/step - loss: 0.0386 - val_loss: 0.0329\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 1s 48ms/step - loss: 0.0389 - val_loss: 0.0329\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 1s 47ms/step - loss: 0.0385 - val_loss: 0.0331\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 1s 49ms/step - loss: 0.0392 - val_loss: 0.0324\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.0385 - val_loss: 0.0314\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.0387 - val_loss: 0.0308\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 1s 48ms/step - loss: 0.0388 - val_loss: 0.0295\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 1s 51ms/step - loss: 0.0384 - val_loss: 0.0294\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.0384 - val_loss: 0.0331\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.0387 - val_loss: 0.0320\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 1s 47ms/step - loss: 0.0381 - val_loss: 0.0298\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 1s 51ms/step - loss: 0.0385 - val_loss: 0.0355\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.0385 - val_loss: 0.0294\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.0381 - val_loss: 0.0353\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.0384 - val_loss: 0.0324\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 1s 47ms/step - loss: 0.0380 - val_loss: 0.0342\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 1s 49ms/step - loss: 0.0382 - val_loss: 0.0344\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 1s 48ms/step - loss: 0.0380 - val_loss: 0.0298\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.0378 - val_loss: 0.0330\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 1s 48ms/step - loss: 0.0378 - val_loss: 0.0407\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.0380 - val_loss: 0.0353\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 1s 56ms/step - loss: 0.0377 - val_loss: 0.0363\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 1s 56ms/step - loss: 0.0377 - val_loss: 0.0401\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 0.0380 - val_loss: 0.0328\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 0.0376 - val_loss: 0.0325\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 1s 53ms/step - loss: 0.0382 - val_loss: 0.0330\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 1s 51ms/step - loss: 0.0379 - val_loss: 0.0340\n"
     ]
    }
   ],
   "source": [
    "#Config\n",
    "\n",
    "pesos=[]\n",
    "neurons=20\n",
    "delta_t=theta[1] # delta_t constante,nao precisa passara para o data gen !\n",
    "rho_input_tensor=tf.convert_to_tensor(x_train)\n",
    "delta_t_tensor=tf.dtypes.cast(tf.convert_to_tensor(delta_t),tf.complex128)\n",
    "\n",
    "\n",
    "model=tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(n_steps,data_dim)))\n",
    "model.add(tf.keras.layers.GRU(neurons,return_sequences=True ,kernel_initializer='random_normal'))\n",
    "model.add(tf.keras.layers.GRU(neurons,return_sequences=True ,kernel_initializer='random_normal'))\n",
    "#model.add(tf.keras.layers.Dropout(.2))\n",
    "model.add(tf.keras.layers.Dense(data_dim))\n",
    "\n",
    "#generator=DataGenerator(x=x_train,y=y_train,batch_size=batch_size)\n",
    "adam = tf.keras.optimizers.RMSprop(lr=0.001,rho=0.9) \n",
    "#Callbacks\n",
    "early_stop =EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=0, \n",
    "                                                     mode='min', baseline=None, restore_best_weights=True)\n",
    "\n",
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: pesos.append(model.layers[1].get_weights()))\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(1),histogram_freq=1)\n",
    "\n",
    "model.compile(loss=Z(delta_t_tensor,d),optimizer=adam)\n",
    "\n",
    "\n",
    "#history=model.fit_generator(generator=DataGenerator(batch_size,x_train,y_train,x_train_0),verbose=True,\n",
    "#                            validation_data=(DataGenerator(batch_size,x_test,y_test,x_test_0)),\n",
    "#                            epochs=100) #callbacks=[early_stop]callbacks=[print_weights,History]\n",
    "history=model.fit(x=x_train,y=data,batch_size=17,validation_data=(x_test,data_test),epochs=50)"
   ]
  },
  {
   "source": [
    "test_size=3 n split=3 loss: 0.0414 - val_loss: 0.0339\n",
    "\n",
    "test_size=1 n split=3 loss: 0.0367 - val_loss: 0.0349\n",
    "\n",
    "test_size=1  n split= 1 loss: 0.0388 - val_loss: 0.0263"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 17, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 17, 8), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (1, 1, 8).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 17, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 17, 8), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (1, 1, 8).\n"
     ]
    }
   ],
   "source": [
    "#Config\n",
    "timesteps=len(estados[0])\n",
    "step_index=len(estados[0])-1\n",
    "\n",
    "divid=np.int(data_dim/2)  \n",
    "H=Qobj(np.zeros((d,d)))\n",
    "timestamps=np.linspace(0,5.6,timesteps)\n",
    "kwargs={'estados_getgama':rho,'estados_test':estados,'timestamps': timestamps,'model':model,'timestep':timesteps, 'step_index':step_index,'divid':divid}  \n",
    "pred=ModeloPredicao(**kwargs)\n",
    "fid_df,norm_df,prediÃ§ao_qutip=pred.predicao()\n",
    "gama_mediatotal=pred.gama_mediatotal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Estado0   Estado1   Estado2   Estado3   Estado4   Estado5   Estado6  \\\n",
       "0   1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "1   0.956433  0.999111  0.999753  0.998508  0.999428  0.998891  0.997241   \n",
       "2   0.969176  0.999034  0.999650  0.999113  0.999308  0.999391  0.999891   \n",
       "3   0.954891  0.998641  0.999416  0.999206  0.997950  0.998584  0.999901   \n",
       "4   0.990080  0.998191  0.999261  0.998941  0.998394  0.998918  0.999843   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "65  0.899969  0.899012  0.904899  0.879527  0.840193  0.897275  0.998250   \n",
       "66  0.886069  0.906526  0.891590  0.884081  0.836915  0.884958  0.998301   \n",
       "67  0.877883  0.885357  0.894704  0.859393  0.822372  0.888811  0.998760   \n",
       "68  0.881324  0.886717  0.884377  0.864971  0.819100  0.872994  0.998960   \n",
       "69  0.870844  0.881318  0.887268  0.831098  0.817882  0.875476  0.998929   \n",
       "\n",
       "     Estado7  \n",
       "0   1.000000  \n",
       "1   0.996342  \n",
       "2   0.998550  \n",
       "3   0.997840  \n",
       "4   0.994362  \n",
       "..       ...  \n",
       "65  0.736801  \n",
       "66  0.736434  \n",
       "67  0.735129  \n",
       "68  0.733972  \n",
       "69  0.734250  \n",
       "\n",
       "[70 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Estado0</th>\n      <th>Estado1</th>\n      <th>Estado2</th>\n      <th>Estado3</th>\n      <th>Estado4</th>\n      <th>Estado5</th>\n      <th>Estado6</th>\n      <th>Estado7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.956433</td>\n      <td>0.999111</td>\n      <td>0.999753</td>\n      <td>0.998508</td>\n      <td>0.999428</td>\n      <td>0.998891</td>\n      <td>0.997241</td>\n      <td>0.996342</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.969176</td>\n      <td>0.999034</td>\n      <td>0.999650</td>\n      <td>0.999113</td>\n      <td>0.999308</td>\n      <td>0.999391</td>\n      <td>0.999891</td>\n      <td>0.998550</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.954891</td>\n      <td>0.998641</td>\n      <td>0.999416</td>\n      <td>0.999206</td>\n      <td>0.997950</td>\n      <td>0.998584</td>\n      <td>0.999901</td>\n      <td>0.997840</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.990080</td>\n      <td>0.998191</td>\n      <td>0.999261</td>\n      <td>0.998941</td>\n      <td>0.998394</td>\n      <td>0.998918</td>\n      <td>0.999843</td>\n      <td>0.994362</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>0.899969</td>\n      <td>0.899012</td>\n      <td>0.904899</td>\n      <td>0.879527</td>\n      <td>0.840193</td>\n      <td>0.897275</td>\n      <td>0.998250</td>\n      <td>0.736801</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>0.886069</td>\n      <td>0.906526</td>\n      <td>0.891590</td>\n      <td>0.884081</td>\n      <td>0.836915</td>\n      <td>0.884958</td>\n      <td>0.998301</td>\n      <td>0.736434</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>0.877883</td>\n      <td>0.885357</td>\n      <td>0.894704</td>\n      <td>0.859393</td>\n      <td>0.822372</td>\n      <td>0.888811</td>\n      <td>0.998760</td>\n      <td>0.735129</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>0.881324</td>\n      <td>0.886717</td>\n      <td>0.884377</td>\n      <td>0.864971</td>\n      <td>0.819100</td>\n      <td>0.872994</td>\n      <td>0.998960</td>\n      <td>0.733972</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>0.870844</td>\n      <td>0.881318</td>\n      <td>0.887268</td>\n      <td>0.831098</td>\n      <td>0.817882</td>\n      <td>0.875476</td>\n      <td>0.998929</td>\n      <td>0.734250</td>\n    </tr>\n  </tbody>\n</table>\n<p>70 rows Ã 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "fid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.06603538-0.07337612j, -0.01283073-0.00842983j,\n",
       "       -0.0063292 +0.01088832j,  0.02613212+0.05505827j])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "gama_mediatotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"/home/andrius/Desktop/RNN Estados ML Jeff/Resultados/+\"+str(datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/andrius/Desktop/RNN Estados ML Jeff/Resultados/+\"+str(datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M\")))\n",
    "\n",
    "\n",
    "np.savetxt('./fidelidades.csv', fid_df)\n",
    "np.savetxt('./tracedist.csv', norm_df)\n",
    "np.savetxt('./Operadores.csv', gama_mediatotal)\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('learning.png')\n",
    "\n",
    "#file=open('./resultados'+str(datetime.datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\"))+'.txt', 'w')\n",
    "file=open(\"/home/andrius/Desktop/RNN Estados ML Jeff/Resultados/+overfitting/resultados.txt\",'w')\n",
    "file.write(\"Coeficientes mÃ©dios encontrados={0}\\n\".format(gama_mediatotal)) \n",
    "file.write(\"Estados tomografados para teste={0}\\n\".format(train_test_split))\n",
    "file.write(\"Steps temporais={0}\\n\".format(n_steps))    \n",
    "file.write(\"Timesteps de validaÃ§Ã£o={0}\\n\".format(timestamps))\n",
    "\n",
    "file.close()\n",
    "\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5e6a15ce28a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd09482e4cf05b440ef352288ce44e3cd0108a509fcb73a7448ef2ddeac19b10c04",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "47709d881f409cf02d74eb50fdac1e95459df83b94de2434af62a049177d54f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}